{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#Code for the Quantile Neural Network Model\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#figure size\n",
    "plt.rcParams[\"figure.figsize\"]=10,10\n",
    "model_data_path ='./model_input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Financial Dataset using log methods\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self,train_dates,eval_dates,df_in,cutoff_year):\n",
    "        self.all_valuation_dates = ['2019-12-31','2018-12-31','2017-12-31','2016-12-31','2015-12-31','2014-12-31','2013-12-31','2012-12-31','2011-12-31','2010-12-31','2009-12-31','2008-12-31','2007-12-31']        \n",
    "        #process the dataframe\n",
    "        df,symbols,marketcaps = self.__process_df__(train_dates,eval_dates,df_in,cutoff_year)\n",
    "        #init with the df\n",
    "        self.model_X, self.model_Y, self.marketcaps_future,self.symbols = self.prepare_data(df,symbols,marketcaps)                        \n",
    "        #normalise the data\n",
    "        self.model_X,self.X_mean,self.X_min_max = self.normalise_X_data(self.model_X)\n",
    "        self.model_Y,self.Y_min,self.Y_min_max = self.normalise_Y_data(self.model_Y)\n",
    "        #create a random sorted index\n",
    "        self.rnd_idx = np.arange(self.symbols.shape[0])\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(self.rnd_idx)        \n",
    "        #shuffle the data\n",
    "        self.marketcaps_future = self.marketcaps_future[self.rnd_idx,:]\n",
    "        self.symbols = self.symbols[self.rnd_idx]\n",
    "        self.model_X = self.model_X[self.rnd_idx,:]\n",
    "        self.model_Y = self.model_Y[self.rnd_idx]        \n",
    "        \n",
    "    def __process_df__(self,train_dates,eval_dates,df,cutoff_year):\n",
    "        df_new = df.loc[df['Source_Year'] <cutoff_year].reset_index(drop=True)\n",
    "        #drop all entries that lack valuation data during the periods of interest\n",
    "        for dt in train_dates+eval_dates:\n",
    "            df_new = df_new.loc[df_new[dt] >0].reset_index(drop=True)\n",
    "        #store the eval date marketcaps seperatly\n",
    "        marketcaps = df_new[eval_dates]\n",
    "        #store the symbols seperatly\n",
    "        symbols = df_new['Symbol']\n",
    "        #remove all the columns we don't want to pass to the model for training\n",
    "        drop_valuation_dates = list(set(self.all_valuation_dates)-set(train_dates))\n",
    "        df_new = df_new.drop(columns=drop_valuation_dates+['Symbol'])\n",
    "        #create and return the dataset\n",
    "        return (df_new,symbols,marketcaps)\n",
    "    \n",
    "    def __get_norm_params__(self):\n",
    "        return ((self.X_mean,self.X_min_max),(self.Y_min,self.Y_min_max))\n",
    "\n",
    "    def normalise_X_data(self,model_data):\n",
    "        log_model_data = torch.log(torch.abs(model_data)+torch.ones(model_data.shape) )*torch.sign(model_data)\n",
    "        min_max_v = torch.max(torch.max(log_model_data,0)[0],0)[0] - torch.min(torch.min(log_model_data,0)[0],0)[0]\n",
    "        mean_v = torch.mean(torch.mean(log_model_data,0),0)\n",
    "        #ensure no divide by zero\n",
    "        min_max_v = min_max_v+ (min_max_v==0)\n",
    "        return ((log_model_data-mean_v)/min_max_v,mean_v,min_max_v)\n",
    "\n",
    "    def normalise_Y_data(self,model_data):\n",
    "        log_model_data = torch.log(model_data)\n",
    "        min_v =torch.min(torch.min(log_model_data,0)[0],0)[0]\n",
    "        min_max_v = torch.max(torch.max(log_model_data,0)[0],0)[0] - min_v\n",
    "        #ensure no divide by zero\n",
    "        min_max_v = min_max_v+ (min_max_v==0)\n",
    "        return ((log_model_data - min_v )/min_max_v,min_v,min_max_v)\n",
    "\n",
    "    def __denormalise__(self,Y_in):\n",
    "        return (torch.exp(Y_in*self.Y_min_max+self.Y_min))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.model_X[idx],self.model_Y[idx],self.marketcaps_future[idx],self.symbols[idx])\n",
    "\n",
    "    def prepare_data(self,df,symbols,marketcaps):\n",
    "        last_symbol=''\n",
    "        count=0\n",
    "        list_X =[]\n",
    "        list_Y =[]\n",
    "        list_marketcaps_future =[]\n",
    "        list_symbols = []\n",
    "        for idx in range(len(df)):\n",
    "            if(symbols[idx]!=last_symbol):\n",
    "                #process a new symbol\n",
    "                count=0\n",
    "                X_seq = []\n",
    "                Y = marketcaps.iloc[idx,0]\n",
    "                mcf = marketcaps.iloc[idx,1:].tolist()\n",
    "                sym = symbols[idx]\n",
    "            #process data\n",
    "            if (count>2):\n",
    "                #already have 3 entries\n",
    "                pass\n",
    "            else:\n",
    "                #process entry\n",
    "                X_seq.append(df.iloc[idx,:].to_list())\n",
    "            #increment the count        \n",
    "            count+=1\n",
    "            last_symbol = symbols[idx]\n",
    "            #store the sequence\n",
    "            if(count==3):\n",
    "                list_X.append(X_seq)\n",
    "                list_Y.append(Y)\n",
    "                list_marketcaps_future.append(mcf)\n",
    "                list_symbols.append(sym)\n",
    "        #convert to Log\n",
    "        return (torch.FloatTensor(list_X),torch.FloatTensor(list_Y),np.array(list_marketcaps_future),np.array(list_symbols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#load the data for the model\n",
    "############################\n",
    "\n",
    "df_data = pd.read_csv(model_data_path,sep='\\t')\n",
    "df_data.drop(columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "#one hot encodings\n",
    "onehot_cols=[]\n",
    "for col in df_data.columns:\n",
    "    if(df_data[col].dtype==np.object and col !=\"Symbol\"):\n",
    "        onehot_cols.append(col)\n",
    "df_data = pd.get_dummies(df_data,columns=onehot_cols,prefix=onehot_cols)\n",
    "\n",
    "#Build the validation dataset\n",
    "train_dates = []#['2008-12-31','2007-12-31']\n",
    "eval_dates = ['2009-12-31','2009-12-31','2010-12-31','2011-12-31','2012-12-31']\n",
    "cutoff_year = 2010\n",
    "dataset_validation = FinancialDataset(train_dates,eval_dates,df_data,cutoff_year)\n",
    "\n",
    "#Build the test dataset\n",
    "train_dates = []#['2015-12-31','2014-12-31']\n",
    "eval_dates = ['2016-12-31','2016-12-31','2017-12-31','2018-12-31','2019-12-31']\n",
    "cutoff_year = 2020\n",
    "dataset_test = FinancialDataset(train_dates,eval_dates,df_data,cutoff_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResults():\n",
    "    def __init__(self):\n",
    "        #a class to hold model results\n",
    "        self.df = None\n",
    "        self.Znames = []\n",
    "\n",
    "    def append(self,Y,Yhat,Z,Symbols,version,type_data):\n",
    "        #create a dataframe and append it to the main dataframe \n",
    "        df_temp = pd.DataFrame(Y.cpu().numpy(),columns=['Y'])\n",
    "        df_temp['Yhat'] = Yhat.detach().cpu().numpy()\n",
    "        #create Z columns\n",
    "        Z_temp=Z.cpu().numpy()\n",
    "        \n",
    "        self.Znames = []\n",
    "        for i in range(Z_temp.shape[1]):\n",
    "            df_temp['Z_' + str(i)] = Z_temp[:,i]\n",
    "            self.Znames.append('Z_' + str(i))\n",
    "        \n",
    "        df_temp['Symbol'] = Symbols\n",
    "        df_temp['Version']=version\n",
    "        df_temp['Type']=type_data\n",
    "        \n",
    "        if (self.df is None):\n",
    "            self.df = df_temp\n",
    "        else:\n",
    "            self.df = self.df.append(df_temp)\n",
    "        return\n",
    "\n",
    "    def append_train(self,Y,Yhat,Z,Symbols,version):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"train\")\n",
    "        return\n",
    "\n",
    "    def append_val(self,Y,Yhat,Z,Symbols,version):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"val\")\n",
    "        return\n",
    "\n",
    "    def append_test(self,Y,Yhat,Z,Symbols,version):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"test\")\n",
    "        return\n",
    "\n",
    "    def save_results(self,file_path):\n",
    "        self.df.to_csv(file_path,index=False)\n",
    "        \n",
    "    def load_results(self,file_path):\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.Znames = []\n",
    "        for i in range(4):\n",
    "            self.Znames.append('Z_' + str(i))\n",
    "        \n",
    "    def get_all_predictions(self):\n",
    "        \n",
    "        #compute the average predictions for each symbol and return \n",
    "        df_avg = self.df.groupby(['Symbol']).mean().reset_index()\n",
    "        return (df_avg['Y'].to_numpy(),df_avg['Yhat'].to_numpy(),df_avg[self.Znames].to_numpy(),df_avg['Symbol'])\n",
    "    \n",
    "    def get_predictions_cap_band(self,cap_from,cap_to):\n",
    "        #compute the average predictions for each symbol and return\n",
    "        df_avg = self.df.groupby(['Symbol']).mean().reset_index()\n",
    "        df_avg = df_avg.sort_values(by='Y',ascending=True).reset_index()\n",
    "        total_entries = len(df_avg)\n",
    "        idx_from = int(cap_from*total_entries)\n",
    "        idx_to = int(cap_to*total_entries)\n",
    "        df_avg = df_avg.iloc[idx_from:idx_to,:].reset_index()\n",
    "        return (df_avg['Y'].to_numpy(),df_avg['Yhat'].to_numpy(),df_avg[self.Znames].to_numpy(),df_avg['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#### Use a neural network to fit the function\n",
    "#######################\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "        \n",
    "#define the neural net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 200\n",
    "        self.n_layers = 3\n",
    "\n",
    "        self.model = nn.ModuleDict({\n",
    "            'LSTM': nn.LSTM(213,self.hidden_dim,num_layers = self.n_layers,batch_first = True,dropout=0.5),\n",
    "            'Linear1': nn.Linear(200,25),\n",
    "            'ReLU': nn.ReLU(),\n",
    "            'Tanh': nn.Tanh(),\n",
    "            'Sigmoid': nn.Sigmoid(),\n",
    "            'Dropout': nn.Dropout(p=0.5),\n",
    "            'Linear2': nn.Linear(25,12),\n",
    "            'Linear3': nn.Linear(12,6),\n",
    "            'Linear4': nn.Linear(6,1)\n",
    "        })\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        #print(batch_size)\n",
    "        h0 = self.init_hidden(batch_size)\n",
    "        c0 = self.init_hidden(batch_size)\n",
    "        #print(hidden.shape)\n",
    "        output, (h_n,c_n) = self.model['LSTM'](x,(h0,c0))\n",
    "        x = output[:,-1,:]\n",
    "        x = self.model['Dropout'](x)\n",
    "        x = self.model['Linear1'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear2'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear3'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear4'](x)\n",
    "        x = torch.exp(x)\n",
    "        x = x.view(-1)\n",
    "        return x\n",
    " \n",
    "#We define the quantile loss function\n",
    "def QuantileLoss(preds, target, quantile=0.25):\n",
    "    err = target - preds    \n",
    "    loss = torch.mean(torch.max((quantile-1) * err, quantile * err).unsqueeze(1))\n",
    "    return loss\n",
    "    \n",
    "def run_model(train_dataset,val_dataset,net_results,run_version):\n",
    "    #train / val / test\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "    val_data_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=True)    \n",
    "    #run the model with this dataset\n",
    "    net = Net().to(device)\n",
    "    params = list(net.parameters())\n",
    "    #specify quantile loss functon\n",
    "    criterion = QuantileLoss\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "    #store the losses as we train\n",
    "    loss_v=[]\n",
    "    loss_t=[]   \n",
    "    #moving average window\n",
    "    window=40\n",
    "    epoch=0\n",
    "    #early stopping condition\n",
    "    while ( (epoch < window) or ( np.mean( np.array(loss_v)[-window:] ) < np.mean( np.array(loss_v)[-(window+1):-1] ) )):\n",
    "        net.train()\n",
    "        t_loss=0\n",
    "        t_count=0\n",
    "        for batch_idx, (X, Y,_,_) in enumerate(train_data_loader):\n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X.to(device))\n",
    "            loss = criterion(outputs, Y.to(device))\n",
    "            t_loss+=loss.item()\n",
    "            t_count+=1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_t.append(t_loss/t_count) \n",
    "        #evaluate the loss on validation set    \n",
    "        v_loss=0\n",
    "        v_count=0\n",
    "        for batch_idx, (X_val, Y_val,_,_) in enumerate(val_data_loader):\n",
    "            v_loss+= criterion(net(X_val.to(device)), Y_val.to(device)).item()\n",
    "            v_count+=1\n",
    "        loss_v.append(v_loss/v_count)\n",
    "        print(f\"epoch = {epoch} train loss = {loss_t[-1]} validation loss = {loss_v[-1]}\")\n",
    "        epoch+=1\n",
    "        \n",
    "    print('Finished Training')\n",
    "    #put model into eval mode\n",
    "    net.eval()\n",
    "    plt.plot(loss_t, \"-b\", label=\"train\")\n",
    "    plt.plot(loss_v , \"-r\", label=\"validation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Train vs validation loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    #generate validation results\n",
    "    for batch_idx, (X_val, Y_val,Z,sym) in enumerate(val_data_loader):\n",
    "        net_results.append_val(\n",
    "            val_dataset.dataset.__denormalise__(Y_val),\n",
    "            val_dataset.dataset.__denormalise__(net(X_val.to(device))),\n",
    "            Z,\n",
    "            sym,\n",
    "            run_version)\n",
    "\n",
    "    #generate train results\n",
    "    for batch_idx, (X_val, Y_val,Z,sym) in enumerate(train_data_loader):\n",
    "        net_results.append_train(\n",
    "            val_dataset.dataset.__denormalise__(Y_val),\n",
    "            val_dataset.dataset.__denormalise__(net(X_val.to(device))),\n",
    "            Z,\n",
    "            sym,\n",
    "            run_version)\n",
    "    #now compute the results for the train and va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a random seed on the split \n",
    "torch.manual_seed(41)\n",
    "\n",
    "CV_Folds = 5\n",
    "CV_data =(None,)*CV_Folds\n",
    "split_sizes = [int(dataset_test.__len__()/CV_Folds)]*CV_Folds\n",
    "split_sizes[-1] = dataset_test.__len__() - (CV_Folds-1)*split_sizes[0]\n",
    "CV_data = torch.utils.data.random_split(dataset_test, split_sizes)\n",
    "CV_data = list(CV_data)\n",
    "net_results = ModelResults()\n",
    "\n",
    "#random runs for each fold\n",
    "Repeat = 20\n",
    "cnt =0\n",
    "for j in range(Repeat):\n",
    "    for i in range(CV_Folds):\n",
    "    \n",
    "        #take first element as validation and rest to train\n",
    "        print(f\"running fold {i+1} on repeat {j+1}\")\n",
    "        train_dataset = torch.utils.data.ConcatDataset(CV_data[1:])\n",
    "        val_dataset = CV_data[0]\n",
    "        print(f\"train dataset size ={train_dataset.__len__()} val dataset size = {val_dataset.__len__()}\")\n",
    "        run_model(train_dataset, val_dataset, net_results,cnt)\n",
    "        cnt+=1\n",
    "        #now change the order of the dataset\n",
    "        CV_data.append(CV_data.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create P&L charts\n",
    "def p_and_l_chart(return_dist,title):\n",
    "    Ys = np.sort(return_dist)\n",
    "    avg = np.mean(Ys)\n",
    "    mdn = np.median(Ys)\n",
    "    avg_log = np.log(1+avg*(avg>=0)) -np.log(1-avg*(avg<0))\n",
    "    Ys_pos = Ys*(Ys>=0)\n",
    "    Ys_neg = Ys*(Ys<0)\n",
    "    Ys_pos = np.log(1+Ys_pos)\n",
    "    Ys_neg = -np.log(1-Ys_neg)\n",
    "    labels = np.arange(Ys.shape[0])\n",
    "    \n",
    "    Y_t = np.array([-10,-1,0,1,10,100,1000,10000,100000,1000000])\n",
    "    Y_t_val = np.log(1+Y_t*(Y_t>=0)) -np.log(1-Y_t*(Y_t<0))\n",
    "    Y_t_labels = (Y_t*100).tolist()\n",
    "    Y_t_labels =[f\"{s:,.0f}%\" for s in Y_t_labels]\n",
    "    plt.yticks(Y_t_val, Y_t_labels)\n",
    "    plt.bar(labels,Ys_pos,width=1,color='green',label=\"Positive Return\")\n",
    "    plt.bar(labels,Ys_neg,width=1,color='red',label=\"Negative Return\")\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "    ax.plot([0., labels[-1]], [avg_log, avg_log], \"k--\",label=\"Mean Return = \" + f\"{avg*100:,.0f}%\")\n",
    "    ax.plot([Ys.shape[0]/2, Ys.shape[0]/2], [-np.log(2), np.log(2)], \"b--\",label=\"Median Return = \"+ f\"{mdn*100:,.0f}%\")\n",
    "    plt.ylabel('Compound Return % after 3 Years')\n",
    "    plt.xlabel('Companies - sorted from low to high return')\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "def training_v_actual_chart(Y,Y_hat,Ycheap,Y_hatcheap,title):\n",
    "    correl = np.corrcoef(Y,Y_hat)[1,0]\n",
    "    plt.scatter(Y, Y_hat, marker=\"o\",color='k',s=1.5,label=\"all companies\")\n",
    "    plt.scatter(Ycheap, Y_hatcheap, marker=\"o\",color='r',s=1.5,label=\"cheap companies\")\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"log(Actual Market Cap)\")\n",
    "    plt.ylabel(\"log(Predicted Market Cap)\")\n",
    "    plt.loglog()\n",
    "    axes = plt.gca()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def n_cheapest(Y,Yhat,n):\n",
    "    #return an array with n values = True for the largest differences\n",
    "    diff = Yhat/Y\n",
    "    #negate the sign to sort descending\n",
    "    idxs = (-diff).argsort()\n",
    "    res = np.zeros(Y.shape).astype(bool)\n",
    "    \n",
    "    for i in range(n):\n",
    "        res[idxs[i]]=True*(Yhat[idxs[i]]>Y[idxs[i]])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net_results = ModelResults()\n",
    "net_results.load_results('test_results_quantile90_model.csv')\n",
    "\n",
    "#Set the company bands\n",
    "bands = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "df_results = None\n",
    "for b in bands:\n",
    "    b_up=b+bands[1]\n",
    "    Y, Y_hat, Z, Symbols = net_results.get_predictions_cap_band(b,b+0.1)\n",
    "    y_ret=[]\n",
    "    y_cap=[]\n",
    "    for i in range(3):\n",
    "        y_ret.append((Z[:,i+1]/Z[:,i]-np.ones(Z.shape[0])))\n",
    "        y_cap.append(Z[:,i])\n",
    "\n",
    "    #create a dataframe with the results\n",
    "    df_temp = pd.DataFrame({'Results':f\"All {b:,.1f}-{b_up:,.1f}\", \n",
    "                               'Companies': Y.shape[0],\n",
    "                                'Yr1 Mean Return %': [f\"{100*np.mean(y_ret[0]):,.0f}%\"], \n",
    "                                'Yr2 Mean Return %': [f\"{100*np.mean(y_ret[1]):,.0f}%\"],                                        \n",
    "                                'Yr3 Mean Return %': [f\"{100*np.mean(y_ret[2]):,.0f}%\"], \n",
    "                              })\n",
    "    if df_results is None:\n",
    "        df_results = df_temp\n",
    "    else:\n",
    "        df_results = df_results.append(df_temp)\n",
    "    all_ret_dist = Z[:,-1]/Y - 1\n",
    "    p_and_l_chart(all_ret_dist,f\"3 Year Results - all companies - market caps {b:,.1f}-{b_up:,.1f}\")\n",
    "\n",
    "    #generate a return chart for different margin levels\n",
    "    cheapest = [Z.shape[0]] #look at the 5% cheapest in each bracket of marketcap on test set and 25% on validation\n",
    "    for cheap in cheapest:\n",
    "        cheap_idx = n_cheapest(Y,Y_hat,cheap)\n",
    "        y_ret=[]\n",
    "        y_cap=[]\n",
    "        for i in range(3):\n",
    "            y_ret.append((Z[:,i+1]/Z[:,i]-np.ones(Z.shape[0]))[cheap_idx])\n",
    "            y_cap.append(Z[:,i][cheap_idx])\n",
    "      \n",
    "        long_ret_dist = all_ret_dist[cheap_idx]\n",
    "        #p_and_l_chart(long_ret_dist,f\"3 Year Results - cheapest {cheap:.0f} companies - market caps {b:,.1f}-{b_up:,.1f}\")\n",
    "        df_temp = pd.DataFrame({'Results':f\"Model {b:,.1f}-{b_up:,.1f} Cheapest = {cheap:.0f}\", \n",
    "                            'Companies': long_ret_dist.shape[0],\n",
    "                            'Yr1 Mean Return %': [f\"{100*np.mean(y_ret[0]):,.0f}%\"], \n",
    "                            'Yr2 Mean Return %': [f\"{100*np.mean(y_ret[1]):,.0f}%\"],                                        \n",
    "                            'Yr3 Mean Return %': [f\"{100*np.mean(y_ret[2]):,.0f}%\"], \n",
    "                           })\n",
    "        df_results = df_results.append(df_temp)\n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "p37workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
