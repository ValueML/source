{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#Mixture Density Network appled to valuation model\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams[\"figure.figsize\"]=10,10\n",
    "model_data_path ='./model_input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice_prob_index(a, axis=1):\n",
    "    r = np.expand_dims(np.random.rand(a.shape[1-axis]), axis=axis)\n",
    "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)\n",
    "\n",
    "def MDN_predict(x,percentile,denorm_fn):\n",
    "    #we make a random sample from the distibution conditional on x\n",
    "    n=int(x.shape[1]/3)\n",
    "    alphas = x[:,0:n]\n",
    "    #second third - variance\n",
    "    variances = x[:,n:(2*n)]\n",
    "    #final third - means\n",
    "    means = x[:,-n:]\n",
    "    sample_size=1000\n",
    "    selections_all =None\n",
    "    for i in range(sample_size):\n",
    "        #compute random selections\n",
    "        idx = random_choice_prob_index(alphas)[:,None]\n",
    "        selections = np.random.normal(np.take_along_axis(means,idx,axis=1),np.take_along_axis(variances,idx,axis=1))\n",
    "        #normalise the selections\n",
    "        selections = denorm_fn(torch.from_numpy(selections).type(torch.FloatTensor)).numpy()\n",
    "        \n",
    "        if (selections_all is None):\n",
    "            selections_all=selections\n",
    "        else:\n",
    "            #append\n",
    "            selections_all=np.hstack((selections_all,selections))\n",
    "    return np.percentile(selections_all,percentile,axis=1)\n",
    "\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self,train_dates,eval_dates,df_in,cutoff_year):\n",
    "        self.all_valuation_dates = ['2019-12-31','2018-12-31','2017-12-31','2016-12-31','2015-12-31','2014-12-31','2013-12-31','2012-12-31','2011-12-31','2010-12-31','2009-12-31','2008-12-31','2007-12-31']\n",
    "        \n",
    "        #process the dataframe\n",
    "        df,symbols,marketcaps = self.__process_df__(train_dates,eval_dates,df_in,cutoff_year)\n",
    "        \n",
    "        #init with the df\n",
    "        self.model_X, self.model_Y, self.marketcaps_future,self.symbols = self.prepare_data(df,symbols,marketcaps)        \n",
    "                \n",
    "        #normalise the data\n",
    "        self.model_X,self.X_mean,self.X_min_max = self.normalise_X_data(self.model_X)\n",
    "        self.model_Y,self.Y_min,self.Y_min_max = self.normalise_Y_data(self.model_Y)\n",
    "        \n",
    "        #create a random sorted index       \n",
    "        self.rnd_idx = np.arange(self.symbols.shape[0])\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(self.rnd_idx)\n",
    "        \n",
    "        #shuffle the data\n",
    "        self.marketcaps_future = self.marketcaps_future[self.rnd_idx,:]\n",
    "        self.symbols = self.symbols[self.rnd_idx]\n",
    "        self.model_X = self.model_X[self.rnd_idx,:]\n",
    "        self.model_Y = self.model_Y[self.rnd_idx]        \n",
    "        \n",
    "    def __process_df__(self,train_dates,eval_dates,df,cutoff_year):\n",
    "        df_new = df.loc[df['Source_Year'] <cutoff_year].reset_index(drop=True)\n",
    "        #drop all entries that lack valuation data during the periods of interest\n",
    "        for dt in train_dates+eval_dates:\n",
    "            df_new = df_new.loc[df_new[dt] >0].reset_index(drop=True)\n",
    "        #store the eval date marketcaps seperatly\n",
    "        marketcaps = df_new[eval_dates]\n",
    "        #store the symbols seperatly\n",
    "        symbols = df_new['Symbol']\n",
    "        #remove all the columns we don't want to pass to the model for training\n",
    "        drop_valuation_dates = list(set(self.all_valuation_dates)-set(train_dates))\n",
    "        df_new = df_new.drop(columns=drop_valuation_dates+['Symbol'])\n",
    "        #create and return the dataset\n",
    "        return (df_new,symbols,marketcaps)\n",
    "    \n",
    "    def __get_norm_params__(self):\n",
    "        return ((self.X_mean,self.X_min_max),(self.Y_min,self.Y_min_max))\n",
    "\n",
    "    def normalise_X_data(self,model_data):\n",
    "        log_model_data = torch.log(torch.abs(model_data)+torch.ones(model_data.shape) )*torch.sign(model_data)\n",
    "        min_max_v = torch.max(torch.max(log_model_data,0)[0],0)[0] - torch.min(torch.min(log_model_data,0)[0],0)[0]\n",
    "        mean_v = torch.mean(torch.mean(log_model_data,0),0)\n",
    "        #ensure no divide by zero\n",
    "        min_max_v = min_max_v+ (min_max_v==0)\n",
    "        return ((log_model_data-mean_v)/min_max_v,mean_v,min_max_v)\n",
    "\n",
    "    def normalise_Y_data(self,model_data):\n",
    "        log_model_data = torch.log(model_data)\n",
    "        min_v =torch.min(torch.min(log_model_data,0)[0],0)[0]\n",
    "        min_max_v = torch.max(torch.max(log_model_data,0)[0],0)[0] - min_v\n",
    "        #ensure no divide by zero\n",
    "        min_max_v = min_max_v+ (min_max_v==0)\n",
    "        return ((log_model_data - min_v )/min_max_v,min_v,min_max_v)\n",
    "\n",
    "    def __denormalise__(self,Y_in):\n",
    "        return (torch.exp(Y_in*self.Y_min_max+self.Y_min))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.model_X[idx],self.model_Y[idx],self.marketcaps_future[idx],self.symbols[idx])\n",
    "\n",
    "    def prepare_data(self,df,symbols,marketcaps):\n",
    "        last_symbol=''\n",
    "        count=0\n",
    "        list_X =[]\n",
    "        list_Y =[]\n",
    "        list_marketcaps_future =[]\n",
    "        list_symbols = []\n",
    "        for idx in range(len(df)):\n",
    "            if(symbols[idx]!=last_symbol):\n",
    "                #process a new symbol\n",
    "                count=0\n",
    "                X_seq = []\n",
    "                Y = marketcaps.iloc[idx,0]\n",
    "                mcf = marketcaps.iloc[idx,1:].tolist()\n",
    "                sym = symbols[idx]\n",
    "            #process data\n",
    "            if (count>2):\n",
    "                #already have 3 entries\n",
    "                pass\n",
    "            else:\n",
    "                #process entry\n",
    "                X_seq.append(df.iloc[idx,:].to_list())\n",
    "            #increment the count        \n",
    "            count+=1\n",
    "            last_symbol = symbols[idx]\n",
    "            #store the sequence\n",
    "            if(count==3):\n",
    "                list_X.append(X_seq)\n",
    "                list_Y.append(Y)\n",
    "                list_marketcaps_future.append(mcf)\n",
    "                list_symbols.append(sym)\n",
    "        #convert to Log\n",
    "        return (torch.FloatTensor(list_X),torch.FloatTensor(list_Y),np.array(list_marketcaps_future),np.array(list_symbols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#load the data for the model\n",
    "############################\n",
    "\n",
    "df_data = pd.read_csv(model_data_path,sep='\\t')\n",
    "df_data.drop(columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "#one hot encodings\n",
    "onehot_cols=[]\n",
    "for col in df_data.columns:\n",
    "    if(df_data[col].dtype==np.object and col !=\"Symbol\"):\n",
    "        onehot_cols.append(col)\n",
    "df_data = pd.get_dummies(df_data,columns=onehot_cols,prefix=onehot_cols)\n",
    "\n",
    "#Build the validation dataset\n",
    "train_dates = []\n",
    "eval_dates = ['2009-12-31','2009-12-31','2010-12-31','2011-12-31','2012-12-31']\n",
    "cutoff_year = 2010\n",
    "dataset_validation = FinancialDataset(train_dates,eval_dates,df_data,cutoff_year)\n",
    "\n",
    "#Build the test dataset\n",
    "train_dates = []\n",
    "eval_dates = ['2016-12-31','2016-12-31','2017-12-31','2018-12-31','2019-12-31']\n",
    "cutoff_year = 2020\n",
    "dataset_test = FinancialDataset(train_dates,eval_dates,df_data,cutoff_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResults():\n",
    "    def __init__(self):\n",
    "        #a class to hold model results\n",
    "        self.df = None\n",
    "        self.Znames = []\n",
    "        self.Ynames = []\n",
    "        self.denorm_fn=None\n",
    "\n",
    "    def append(self,Y,Yhat,Z,Symbols,version,type_data,denorm_fn):\n",
    "        self.denorm_fn=denorm_fn\n",
    "        #create a dataframe and append it to the main dataframe \n",
    "        df_temp = pd.DataFrame(Y.cpu().numpy(),columns=['Y'])\n",
    "        Y_temp = Yhat.detach().cpu().numpy()\n",
    "        \n",
    "        self.Ynames = []\n",
    "        for i in range(Y_temp.shape[1]):\n",
    "            df_temp['Y_' + str(i)] = Y_temp[:,i]\n",
    "            self.Ynames.append('Y_' + str(i))\n",
    "\n",
    "        #create Z columns\n",
    "        Z_temp=Z.cpu().numpy()\n",
    "        \n",
    "        self.Znames = []\n",
    "        for i in range(Z_temp.shape[1]):\n",
    "            df_temp['Z_' + str(i)] = Z_temp[:,i]\n",
    "            self.Znames.append('Z_' + str(i))\n",
    "        \n",
    "        df_temp['Symbol'] = Symbols\n",
    "        df_temp['Version']=version\n",
    "        df_temp['Type']=type_data\n",
    "        \n",
    "        if (self.df is None):\n",
    "            self.df = df_temp\n",
    "        else:\n",
    "            self.df = self.df.append(df_temp)\n",
    "        return\n",
    "\n",
    "    def append_train(self,Y,Yhat,Z,Symbols,version,denorm_fn):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"train\",denorm_fn)\n",
    "        return\n",
    "\n",
    "    def append_val(self,Y,Yhat,Z,Symbols,version,denorm_fn):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"val\",denorm_fn)\n",
    "        return\n",
    "\n",
    "    def append_test(self,Y,Yhat,Z,Symbols,version,denorm_fn):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"test\",denorm_fn)\n",
    "        return\n",
    "\n",
    "    def save_results(self,file_path):\n",
    "        self.df.to_csv(file_path,index=False)\n",
    "        \n",
    "    def get_all_predictions(self):\n",
    "        \n",
    "        #compute the average predictions for each symbol and return \n",
    "        df_avg = self.df.groupby(['Symbol']).mean().reset_index()\n",
    "        return (df_avg['Y'].to_numpy(),df_avg['Yhat'].to_numpy(),df_avg[self.Znames].to_numpy(),df_avg['Symbol'])\n",
    "    \n",
    "    def get_predictions_cap_band(self,cap_from,cap_to,percentile):\n",
    "        #compute the average predictions for each symbol and return \n",
    "        df_avg = self.df.copy()\n",
    "        #now compute the distibution values at the specified return period\n",
    "        #get the numpy distributions\n",
    "        df_avg['predictions']=MDN_predict(df_avg[self.Ynames].to_numpy(),percentile,self.denorm_fn)\n",
    "        df_avg['Y'] =self.denorm_fn(torch.from_numpy(df_avg['Y'].to_numpy()).type(torch.FloatTensor)).numpy()\n",
    "        df_avg = df_avg.groupby(['Symbol']).mean().reset_index()        \n",
    "        df_avg = df_avg.sort_values(by='Y',ascending=True).reset_index()\n",
    "        total_entries = len(df_avg)\n",
    "        idx_from = int(cap_from*total_entries)\n",
    "        idx_to = int(cap_to*total_entries)\n",
    "        df_avg = df_avg.iloc[idx_from:idx_to,:].reset_index()\n",
    "        return (df_avg['Y'].to_numpy(),df_avg['predictions'].to_numpy(),df_avg[self.Znames].to_numpy(),df_avg['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#### Use a neural network to fit the function\n",
    "#######################\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "  \n",
    "def MDN_Output_Layer(x):\n",
    "    #we convert the final layer of the network into the MDN outputs\n",
    "    with torch.no_grad():\n",
    "        n=int(x.shape[1]/3)\n",
    "    #first third - softmax layer for the alpha weightings\n",
    "    alphas=torch.nn.functional.softmax(x[:,0:n],1)\n",
    "    #second third - exponential layer for the variance\n",
    "    variance =torch.exp(x[:,n:(2*n)])\n",
    "    #final third - unchanged, for the means\n",
    "    means = x[:,-n:]\n",
    "    x_adjusted = torch.cat((alphas,variance,means),1)\n",
    "    return x_adjusted\n",
    "\n",
    "def MDN_loss(x,y):\n",
    "    #we compute the MDN loss using a custom loss function\n",
    "    with torch.no_grad():\n",
    "        n=int(x.shape[1]/3)\n",
    "    #first third - alpha weightings\n",
    "    alphas = x[:,0:n]\n",
    "    #second third - variance\n",
    "    variances = x[:,n:(2*n)]\n",
    "    #final third - means\n",
    "    means = x[:,-n:]\n",
    "    #compute the loss\n",
    "    gaussians = torch.exp(-((y.view(-1,1).repeat(1,n) - means)**2)/(2*variances**2))/(variances * (2*3.1415927410125732)**(0.5))\n",
    "    #return the loss\n",
    "    loss = -torch.sum(torch.log(torch.sum(gaussians*alphas,1)))/x.shape[0]\n",
    "    return loss\n",
    "\n",
    "#inputs are sequences of 3,223\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 200\n",
    "        self.n_layers = 3\n",
    "\n",
    "        self.model = nn.ModuleDict({\n",
    "            'LSTM': nn.LSTM(213,self.hidden_dim,num_layers = self.n_layers,batch_first = True,dropout=0.5),\n",
    "            'Linear1': nn.Linear(200,50),\n",
    "            'ReLU': nn.ReLU(),\n",
    "            'Tanh': nn.Tanh(),\n",
    "            'Sigmoid': nn.Sigmoid(),\n",
    "            'Dropout': nn.Dropout(p=0.5),\n",
    "            'Linear2': nn.Linear(50,50),\n",
    "            'Linear3': nn.Linear(50,50),\n",
    "            'Linear4': nn.Linear(50,13*3)\n",
    "        })\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size)\n",
    "        c0 = self.init_hidden(batch_size)\n",
    "        output, (h_n,c_n) = self.model['LSTM'](x,(h0,c0))\n",
    "        x = output[:,-1,:]\n",
    "        x = self.model['Dropout'](x)\n",
    "        x = self.model['Linear1'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear2'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear3'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear4'](x)\n",
    "        x = MDN_Output_Layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def run_model(train_dataset,val_dataset,net_results,run_version):\n",
    "    #train / val / test\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "    val_data_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=True)    \n",
    "    #run the model with this dataset\n",
    "    net = Net().to(device)\n",
    "    params = list(net.parameters())\n",
    "    criterion = MDN_loss\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)#0.005\n",
    "\n",
    "    loss_v=[]\n",
    "    loss_t=[]\n",
    "    \n",
    "    #moving average window\n",
    "    window=40\n",
    "    epoch=0\n",
    "    while ( (epoch < window) or ( np.mean( np.array(loss_v)[-window:] ) < np.mean( np.array(loss_v)[-(window+1):-1] ) )):\n",
    "        net.train()\n",
    "        t_loss=0\n",
    "        t_count=0\n",
    "        for batch_idx, (X, Y,_,_) in enumerate(train_data_loader):\n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X.to(device))\n",
    "            loss = criterion(outputs, Y.to(device))\n",
    "            t_loss+=loss.item()\n",
    "            t_count+=1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_t.append(t_loss/t_count) \n",
    "        #evaluate the loss on validation set    \n",
    "        v_loss=0\n",
    "        v_count=0\n",
    "        for batch_idx, (X_val, Y_val,_,_) in enumerate(val_data_loader):\n",
    "            v_loss+= criterion(net(X_val.to(device)), Y_val.to(device)).item()\n",
    "            v_count+=1\n",
    "        loss_v.append(v_loss/v_count)\n",
    "        print(f\"epoch = {epoch} train loss = {loss_t[-1]} validation loss = {loss_v[-1]}\")\n",
    "        epoch+=1\n",
    "        \n",
    "    print('Finished Training')\n",
    "    #put model into eval mode\n",
    "    net.eval()\n",
    "    plt.plot(loss_t, \"-b\", label=\"train\")\n",
    "    plt.plot(loss_v , \"-r\", label=\"validation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Train vs validation loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    #generate validation results\n",
    "    for batch_idx, (X_val, Y_val,Z,sym) in enumerate(val_data_loader):\n",
    "        net_results.append_val(\n",
    "            Y_val,\n",
    "            net(X_val.to(device)),\n",
    "            Z,\n",
    "            sym,\n",
    "            run_version,val_dataset.dataset.__denormalise__)\n",
    "\n",
    "    #generate train results\n",
    "    for batch_idx, (X_val, Y_val,Z,sym) in enumerate(train_data_loader):\n",
    "        net_results.append_train(\n",
    "            Y_val,\n",
    "            net(X_val.to(device)),\n",
    "            Z,\n",
    "            sym,\n",
    "            run_version,val_dataset.dataset.__denormalise__)\n",
    "    #now compute the results for the train and va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a random seed on the split \n",
    "torch.manual_seed(41)\n",
    "\n",
    "CV_Folds = 5\n",
    "CV_data =(None,)*CV_Folds\n",
    "split_sizes = [int(dataset_test.__len__()/CV_Folds)]*CV_Folds\n",
    "split_sizes[-1] = dataset_test.__len__() - (CV_Folds-1)*split_sizes[0]\n",
    "CV_data = torch.utils.data.random_split(dataset_test, split_sizes)\n",
    "CV_data = list(CV_data)\n",
    "net_results = ModelResults()\n",
    "\n",
    "#random runs for each fold\n",
    "Repeat = 1\n",
    "cnt =0\n",
    "for j in range(Repeat):\n",
    "    for i in range(CV_Folds):\n",
    "    \n",
    "        #take first element as validation and rest to train\n",
    "        print(f\"running fold {i+1} on repeat {j+1}\")\n",
    "        train_dataset = torch.utils.data.ConcatDataset(CV_data[1:])\n",
    "        val_dataset = CV_data[0]\n",
    "        print(f\"train dataset size ={train_dataset.__len__()} val dataset size = {val_dataset.__len__()}\")\n",
    "        run_model(train_dataset, val_dataset, net_results,cnt)\n",
    "        cnt+=1\n",
    "        #now change the order of the dataset\n",
    "        CV_data.append(CV_data.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create P&L charts\n",
    "def p_and_l_chart(return_dist,title):\n",
    "    Ys = np.sort(return_dist)\n",
    "    avg = np.mean(Ys)\n",
    "    mdn = np.median(Ys)\n",
    "    avg_log = np.log(1+avg*(avg>=0)) -np.log(1-avg*(avg<0))\n",
    "    Ys_pos = Ys*(Ys>=0)\n",
    "    Ys_neg = Ys*(Ys<0)\n",
    "    Ys_pos = np.log(1+Ys_pos)\n",
    "    Ys_neg = -np.log(1-Ys_neg)\n",
    "    labels = np.arange(Ys.shape[0])\n",
    "    \n",
    "    Y_t = np.array([-10,-1,0,1,10,100,1000,10000,100000,1000000])\n",
    "    Y_t_val = np.log(1+Y_t*(Y_t>=0)) -np.log(1-Y_t*(Y_t<0))\n",
    "    Y_t_labels = (Y_t*100).tolist()\n",
    "    Y_t_labels =[f\"{s:,.0f}%\" for s in Y_t_labels]\n",
    "    plt.yticks(Y_t_val, Y_t_labels)\n",
    "    plt.bar(labels,Ys_pos,width=1,color='green',label=\"Positive Return\")\n",
    "    plt.bar(labels,Ys_neg,width=1,color='red',label=\"Negative Return\")\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "    ax.plot([0., labels[-1]], [avg_log, avg_log], \"k--\",label=\"Mean Return = \" + f\"{avg*100:,.0f}%\")\n",
    "    ax.plot([Ys.shape[0]/2, Ys.shape[0]/2], [-np.log(2), np.log(2)], \"b--\",label=\"Median Return = \"+ f\"{mdn*100:,.0f}%\")\n",
    "    plt.ylabel('Compound Return % after 3 Years')\n",
    "    plt.xlabel('Companies - sorted from low to high return')\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "def training_v_actual_chart(Y,Y_hat,Ycheap,Y_hatcheap,title):\n",
    "    correl = np.corrcoef(Y,Y_hat)[1,0]\n",
    "    plt.scatter(Y, Y_hat, marker=\"o\",color='k',s=1.5,label=\"all companies\")\n",
    "    plt.scatter(Ycheap, Y_hatcheap, marker=\"o\",color='r',s=1.5,label=\"cheap companies\")\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"log(Actual Market Cap)\")\n",
    "    plt.ylabel(\"log(Predicted Market Cap)\")\n",
    "    plt.loglog()\n",
    "    axes = plt.gca()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def n_cheapest(Y,Yhat,n):\n",
    "    #return an array with n values = True for the largest differences\n",
    "    diff = Yhat/Y\n",
    "    #negate the sign to sortt descending\n",
    "    idxs = (-diff).argsort()\n",
    "    res = np.zeros(Y.shape).astype(bool)\n",
    "    \n",
    "    for i in range(n):\n",
    "        res[idxs[i]]=True\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y, Y_hat, Z, Symbols = net_results.get_all_predictions()\n",
    "percentile =0.5\n",
    "#define the company size bands\n",
    "bands = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "df_results = None\n",
    "for b in bands:\n",
    "    b_up=b+bands[1]\n",
    "    Y, Y_hat, Z, Symbols = net_results.get_predictions_cap_band(b,b+0.1,percentile)\n",
    "    y_ret=[]\n",
    "    y_cap=[]\n",
    "    for i in range(3):\n",
    "        y_ret.append((Z[:,i+1]/Z[:,i]-np.ones(Z.shape[0])))\n",
    "        y_cap.append(Z[:,i])\n",
    "\n",
    "    #create a dataframe with the results\n",
    "    df_temp = pd.DataFrame({'Results':f\"All {b:,.1f}-{b_up:,.1f}\", \n",
    "                               'Companies': Y.shape[0],\n",
    "                                'Yr1 Mean Return %': [f\"{100*np.mean(y_ret[0]):,.0f}%\"], \n",
    "                                'Yr2 Mean Return %': [f\"{100*np.mean(y_ret[1]):,.0f}%\"],                                        \n",
    "                                'Yr3 Mean Return %': [f\"{100*np.mean(y_ret[2]):,.0f}%\"], \n",
    "                              })\n",
    "    if df_results is None:\n",
    "        df_results = df_temp\n",
    "    else:\n",
    "        df_results = df_results.append(df_temp)\n",
    "    all_ret_dist = Z[:,-1]/Y - 1\n",
    "    p_and_l_chart(all_ret_dist,f\"3 Year Results - all companies - market caps {b:,.1f}-{b_up:,.1f}\")\n",
    "\n",
    "    #generate a return chart for different margin levels\n",
    "    cheapest = [40] #look at the 5% cheapest in each bracket of marketcap on test set and 25% on validation\n",
    "    for cheap in cheapest:\n",
    "        cheap_idx = n_cheapest(Y,Y_hat,cheap)\n",
    "        y_ret=[]\n",
    "        y_cap=[]\n",
    "        for i in range(3):\n",
    "            y_ret.append((Z[:,i+1]/Z[:,i]-np.ones(Z.shape[0]))[cheap_idx])\n",
    "            y_cap.append(Z[:,i][cheap_idx])\n",
    "        \n",
    "        #generate a scatter chart of the predictions vs actual market caps\n",
    "        training_v_actual_chart(Y,Y_hat,Y[cheap_idx],Y_hat[cheap_idx],f\"Actual vs Predicted Market Caps in {b:,.1f}-{b_up:,.1f}\")\n",
    "      \n",
    "        long_ret_dist = all_ret_dist[cheap_idx]\n",
    "        p_and_l_chart(long_ret_dist,f\"3 Year Results - cheapest {cheap:.0f} companies - market caps {b:,.1f}-{b_up:,.1f}\")\n",
    "        df_temp = pd.DataFrame({'Results':f\"Model {b:,.1f}-{b_up:,.1f} Cheapest = {cheap:.0f}\", \n",
    "                            'Companies': long_ret_dist.shape[0],\n",
    "                            'Yr1 Mean Return %': [f\"{100*np.mean(y_ret[0]):,.0f}%\"], \n",
    "                            'Yr2 Mean Return %': [f\"{100*np.mean(y_ret[1]):,.0f}%\"],                                        \n",
    "                            'Yr3 Mean Return %': [f\"{100*np.mean(y_ret[2]):,.0f}%\"], \n",
    "                           })\n",
    "        df_results = df_results.append(df_temp)\n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "p37workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
