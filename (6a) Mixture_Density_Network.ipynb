{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#We recreate the MDN implementation described by Bishop C. [1994]\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "plt.rcParams[\"figure.figsize\"]=15,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the toy model inverse problem\n",
    "def inv_model(t):\n",
    "    #create a an array of uniform random variables\n",
    "    eps = np.random.uniform(-0.1,0.1,t.shape)\n",
    "    return t + 0.3*np.sin(2*np.pi*t) + eps\n",
    "    \n",
    "#we create a dataset\n",
    "X_train = np.random.uniform(0,1,(4000))\n",
    "Y_train = torch.Tensor(inv_model(X_train)).unsqueeze(1)\n",
    "X_train = torch.Tensor(X_train).unsqueeze(1)\n",
    "\n",
    "X_val = np.random.uniform(0,1,(4000))\n",
    "Y_val = torch.Tensor(inv_model(X_val)).unsqueeze(1)\n",
    "X_val = torch.Tensor(X_val).unsqueeze(1)\n",
    "\n",
    "X_test = np.random.uniform(0,1,(4000))\n",
    "Y_test = torch.Tensor(inv_model(X_test)).unsqueeze(1)\n",
    "X_test = torch.Tensor(X_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#load data (for previously generated data)\n",
    "#version = \"uniform\"\n",
    "version = \"gaussian\"\n",
    "#version = \"gamma\"\n",
    "X_train = torch.load(version+'_X_train.pt')\n",
    "Y_train = torch.load(version+'_Y_train.pt')\n",
    "X_val = torch.load(version+'_X_val.pt')\n",
    "Y_val = torch.load(version+'_Y_val.pt')\n",
    "X_test = torch.load(version+'_X_test.pt')\n",
    "Y_test = torch.load(version+'_Y_test.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the toy model\n",
    "#predict on test set\n",
    "X_test_sorted,_=torch.sort(X_test,0)\n",
    "Y_test_predict = net(X_test_sorted).detach().numpy()\n",
    "x = np.linspace(0,1,5000)\n",
    "y = inv_model(x)\n",
    "plt.scatter(x,y , color='grey', marker='o',s=5,facecolors='none', label=\"true function\")\n",
    "plt.plot(X_test_sorted,Y_test_predict,  \"black\", label=\"NN\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"True vs. Approximated function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#### First we use a regular neural network to fit the inverse function\n",
    "#######################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "params = list(net.parameters())\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "loss_v=[]\n",
    "loss_t=[]\n",
    "\n",
    "traindata = TensorDataset( X_train,Y_train )\n",
    "dataloader = DataLoader(traindata, batch_size= 500, shuffle=True)\n",
    "    \n",
    "for epoch in range(5000):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    ######NOTE THAT WE SWAP THE X AND Y HERE #########\n",
    "    for batch_idx, (Y, X) in enumerate(dataloader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(input=outputs, target=Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_t.append(loss.item()) \n",
    "        loss_v.append(criterion(net(Y_val), X_val).item())\n",
    "    if epoch%100 ==0:\n",
    "        print(f\"epoch = {epoch} train loss = {loss_t[-1]} validation loss = {loss_v[-1]}\")\n",
    "        \n",
    "print('Finished Training')\n",
    "\n",
    "plt.plot(loss_t, \"-b\", label=\"train\")\n",
    "plt.plot(loss_v , \"-r\", label=\"validation\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Train vs validation loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the toy model\n",
    "#predict on test set\n",
    "Y_test_sorted,_=torch.sort(Y_test,0)\n",
    "X_test_predict = net(Y_test_sorted).detach().numpy()\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "y = inv_model(x)\n",
    "plt.scatter(y,x , color='grey', marker='o',s=5,facecolors='none', label=\"true function\")\n",
    "plt.plot(Y_test_sorted,X_test_predict,  \"black\", label=\"NN\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"True vs. Approximated function - On Input and Target Variables Interchanged\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#### Mixture Density Network to fit the inverse function\n",
    "#######################\n",
    "\n",
    "def MDN_Output_Layer(x):\n",
    "    #we convert the final layer of the network into the MDN outputs\n",
    "    with torch.no_grad():\n",
    "        n=int(x.shape[1]/3)\n",
    "    #first third - softmax layer for the alpha weightings\n",
    "    alphas=torch.nn.functional.softmax(x[:,0:n],1)\n",
    "    #second third - exponential layer for the variance\n",
    "    variance =torch.exp(x[:,n:(2*n)])\n",
    "    #final third - unchanged, for the means\n",
    "    means = x[:,-n:]\n",
    "    x_adjusted = torch.cat((alphas,variance,means),1)\n",
    "    return x_adjusted\n",
    "\n",
    "def MDN_loss(x,y):\n",
    "    #we compute the MDN loss using a custom loss function\n",
    "    with torch.no_grad():\n",
    "        n=int(x.shape[1]/3)\n",
    "    #first third - alpha weightings\n",
    "    alphas = x[:,0:n]\n",
    "    #second third - variance\n",
    "    variances = x[:,n:(2*n)]\n",
    "    #final third - means\n",
    "    means = x[:,-n:]\n",
    "    #compute the loss\n",
    "    gaussians = torch.exp(-((y.repeat(1,n) - means)**2)/(2*variances**2))/(variances * (2*3.1415927410125732)**(0.5))\n",
    "    #return the loss\n",
    "    loss = -torch.sum(torch.log(torch.sum(gaussians*alphas,1)))/x.shape[0]\n",
    "    return loss\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 34*3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        #apply the custom output layer\n",
    "        return MDN_Output_Layer(x)\n",
    "\n",
    "net = Net()\n",
    "params = list(net.parameters())\n",
    "\n",
    "#We apply the MDN loss function\n",
    "criterion = MDN_loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "loss_v=[]\n",
    "loss_t=[]\n",
    "    \n",
    "traindata = TensorDataset( X_train,Y_train )\n",
    "dataloader = DataLoader(traindata, batch_size= 4000, shuffle=True)\n",
    "    \n",
    "for epoch in range(6000):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    ######NOTE THAT WE SWAP THE X AND Y HERE #########\n",
    "    for batch_idx, (Y, X) in enumerate(dataloader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_t.append(loss.item()) \n",
    "    \n",
    "        #evaluate the loss on validation set    \n",
    "        loss_v.append(criterion(net(Y_val), X_val).item())\n",
    "    if epoch%100 ==0:\n",
    "        print(f\"epoch = {epoch} train loss = {loss_t[-1]} validation loss = {loss_v[-1]}\")\n",
    "        \n",
    "print('Finished Training')\n",
    "\n",
    "plt.plot(loss_t, \"-b\", label=\"train\")\n",
    "plt.plot(loss_v , \"-r\", label=\"validation\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Train vs validation loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice_prob_index(a, axis=1):\n",
    "    r = np.expand_dims(np.random.rand(a.shape[1-axis]), axis=axis)\n",
    "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)\n",
    "\n",
    "def MDN_predict(x):\n",
    "    #we make a random sample from the distibution conditional on x\n",
    "    n=int(x.shape[1]/3)\n",
    "    alphas = x[:,0:n]\n",
    "    #second third - variance\n",
    "    variances = x[:,n:(2*n)]\n",
    "    #final third - means\n",
    "    means = x[:,-n:]\n",
    "    idx = random_choice_prob_index(alphas)[:,None]\n",
    "    selections = np.random.normal(np.take_along_axis(means,idx,axis=1),np.take_along_axis(variances,idx,axis=1))\n",
    "    ###################\n",
    "    #Optional code to plot the mode fit\n",
    "    #idx = np.argmax(alphas,1)[:,None]\n",
    "    #selections = np.take_along_axis(means,np.argmax(alphas,1)[:,None],axis=1)\n",
    "    ###################\n",
    "    return selections\n",
    "\n",
    "#plot the toy model\n",
    "X_test_predict_MDN = net(Y_test).detach().numpy()\n",
    "X_test_predict = MDN_predict(X_test_predict_MDN)\n",
    "plt.scatter(Y_test,X_test , color='red', marker='o',s=5,facecolors='none', label=\"True Function\")\n",
    "plt.scatter(Y_test,X_test_predict, color='blue', marker='o',s=5,facecolors='none', label=\"MDN\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"MDN True vs. Approximated function - On Input and Target Variables Interchanged - \" + version)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to compute the cdf by randomly picking alphas and then corresponding gaussians.\n",
    "def mdn_pdf(t,x):\n",
    "    n=int(x.shape[1]/3)\n",
    "    alphas = x[:,0:n]\n",
    "    #second third - variance\n",
    "    variances = x[:,n:(2*n)]\n",
    "    #final third - means\n",
    "    means = x[:,-n:]\n",
    "    norm_pdfs = scipy.stats.norm.pdf(t,means, variances)\n",
    "    return np.sum(alphas*norm_pdfs,1)\n",
    "\n",
    "def mdn_sim_cdf(x):\n",
    "    #simulate a cdf for the the MDN\n",
    "    res=1000\n",
    "    cdf = None\n",
    "    for i in range(res):\n",
    "        this_pdf = (mdn_pdf((i/(res-1))*np.ones((p.shape[0],1)),x)).reshape(-1,1)        \n",
    "        if (cdf is None):\n",
    "            cdf=this_pdf\n",
    "        else:\n",
    "            cdf=np.concatenate((cdf,this_pdf),axis=1)\n",
    "    #now we have a bunch of pdfs, create a cdf\n",
    "    cdf = np.cumsum(cdf,1)\n",
    "    cdf = cdf/cdf[:,-1].reshape(-1,1)\n",
    "    return cdf\n",
    "    \n",
    "#compute the scores\n",
    "# the score functions\n",
    "#average negative log predictive density (NLPD) (Good, 1952)\n",
    "def NLPD(x_obs,y_obs,p):\n",
    "    #compute the pdfs for each y|x\n",
    "    prob = mdn_pdf(x_obs.view(-1,1).numpy(),p).reshape(-1,1)\n",
    "    #set the minimum to 1/n, taking n=4000 to avoid infinite predictions\n",
    "    prob = (prob<1/p.shape[0])*1/p.shape[0]+prob*(prob>=1/p.shape[0])\n",
    "    return -np.sum(np.log(prob))/prob.shape[0]\n",
    "\n",
    "#MAE to the median of the distribution \n",
    "def MAE(x_obs,y_obs,cdf):\n",
    "    res=cdf.shape[1]\n",
    "    medians = cdf*(cdf<=0.5)\n",
    "    medians = (np.argmax(medians,1)/(res-1)).reshape(-1,1)\n",
    "    return np.sum(np.abs(medians-x_obs.view(-1,1).numpy()))/medians.shape[0]\n",
    "\n",
    "#Continuous Ranked Probability Score or the CRPS. The CRPS (Gneiting & Raftery, 2004)\n",
    "def CRPS(x_obs,y_obs,cdf_in):\n",
    "    #resolution \n",
    "    res = cdf_in.shape[1]\n",
    "    #we get the cdf values at each point\n",
    "    cdf = None\n",
    "    for i in range(res):\n",
    "        this_cdf = cdf_in[:,i].reshape(-1,1)\n",
    "        if (cdf is None):\n",
    "            cdf=this_cdf\n",
    "        else:\n",
    "            cdf=np.concatenate((cdf,this_cdf),axis=1)\n",
    "    cdf_lhs = cdf*cdf*(cdf<=x_obs.view(-1,1).numpy())\n",
    "    cdf_rhs = (cdf-1)*(cdf-1)*(cdf>=x_obs.view(-1,1).numpy())\n",
    "    \n",
    "    #now compute the approx area using trapezium rule\n",
    "    cdf_lhs = np.sum(cdf_lhs[:,:-1] +cdf_lhs[:,1:],1)/(res*2)\n",
    "    cdf_rhs = np.sum(cdf_rhs[:,:-1] +cdf_rhs[:,1:],1)/(res*2)\n",
    "    crps = np.sum(cdf_lhs+cdf_rhs)/p.shape[0]\n",
    "    return crps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = net(Y_test).detach().numpy()\n",
    "mdn_cdf = mdn_sim_cdf(p)\n",
    "print(f\"NLPD = {NLPD(X_test,Y_test,p)}\")\n",
    "print(f\"MAE = {MAE(X_test,Y_test,mdn_cdf)}\")\n",
    "print(f\"CRPS = {CRPS(X_test,Y_test,mdn_cdf)}\")\n",
    "\n",
    "Y_density = torch.linspace(0.0, 1.0,1000).view(-1,1)\n",
    "X_density = torch.linspace(0.0, 1.0,1000).view(-1,1)\n",
    "X_density_MDN = net(Y_density).detach()\n",
    "X, Y = torch.meshgrid(X_density.view(-1), Y_density.view(-1))\n",
    "Z = torch.zeros(X.shape)\n",
    "for i in range(1000):\n",
    "    Z[:,i] = torch.from_numpy(mdn_pdf(X[i,:].view(-1,1).numpy(),X_density_MDN.numpy()))\n",
    "    Z[:,i] = Z[:,i]*(Z[:,i]>0)\n",
    "    Z[:,i] = Z[:,i]/torch.sum(Z[:,i])\n",
    "plt.contourf(X, Y, Z, 5, cmap='RdGy')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Probability Density', rotation=90)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"MDN - \" + version)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "p37workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
