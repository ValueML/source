{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#Bezier Density Network - constrained version - appled to valuation model\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "plt.rcParams[\"figure.figsize\"]=10,10\n",
    "model_data_path ='./model_input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the Bezier functions\n",
    "def bezier(n,t):\n",
    "    l = t.shape[0]\n",
    "    b = torch.zeros(l,n+1).to(device)\n",
    "    for i in range(n+1):\n",
    "        b[:,i] = (math.factorial(n)/(math.factorial(i)*math.factorial(n-i)))*(t**i)*(1-t)**(n-i)\n",
    "    return b\n",
    "\n",
    "def bezier_pdf(t,p):\n",
    "    n = int(p.shape[1]/2)\n",
    "    b = bezier(n-2,t)     \n",
    "    x_delta = p[:,1:n] - p[:,:(n-1)]\n",
    "    z_delta = p[:,(n+1):] - p[:,n:-1]\n",
    "    denom=torch.sum(b*x_delta,1)\n",
    "    prob = torch.sum(b*z_delta,1)/denom\n",
    "    return prob\n",
    "\n",
    "def bezier_cdf(t,p):\n",
    "    n = int(p.shape[1]/2)\n",
    "    z = p[:,n:]\n",
    "    x = p[:,:n]\n",
    "    b = bezier(n-1,t)\n",
    "    prob = torch.sum(b*z,1)\n",
    "    return prob\n",
    "\n",
    "def bezier_inv_cdf(probs,p):\n",
    "    #now we need to find the t values that correspond to these selections\n",
    "    l = p.shape[0]\n",
    "    threshold =0.000001\n",
    "    max_loops=200\n",
    "    t_min=torch.zeros(l).to(device).view(-1,1)\n",
    "    t_max=torch.ones(l).to(device).view(-1,1)\n",
    "    while((max_loops>0) and (torch.sum(t_max-t_min)>=threshold)):\n",
    "        max_loops-=1\n",
    "        t_mid = (t_min+t_max)/2\n",
    "        prob_mid = bezier_cdf(t_mid.view(-1),p).view(-1,1)\n",
    "        t_max = t_max*(probs>=prob_mid)+t_mid*(probs<prob_mid)\n",
    "        t_min = t_min*(probs<prob_mid)+t_mid*(probs>=prob_mid)\n",
    "    #now we have the t values convert these into x's and return    \n",
    "    x=t_mid\n",
    "    return x\n",
    "\n",
    "def bezier_rnd(p):\n",
    "    #takes a vector of bezier curves and returns random selections\n",
    "    #first we get random selections in 0,1\n",
    "    l = p.shape[0]\n",
    "    probs = torch.FloatTensor(l).uniform_(0.0, 1.0).to(device)\n",
    "    #now we need to find the t values that correspond to these selections\n",
    "    threshold =0.000001\n",
    "    max_loops=200\n",
    "    t_min=torch.zeros(l).to(device)\n",
    "    t_max=torch.ones(l).to(device)\n",
    "    while((max_loops>0) and (torch.sum(t_max-t_min)>=threshold)):\n",
    "        max_loops-=1\n",
    "        t_mid = (t_min+t_max)/2\n",
    "        prob_mid = bezier_cdf(t_mid,p)\n",
    "        t_max = t_max*(probs>=prob_mid)+t_mid*(probs<prob_mid)\n",
    "        t_min = t_min*(probs<prob_mid)+t_mid*(probs>=prob_mid)\n",
    "    #now we have the t values convert these into x's and return    \n",
    "    x=t_mid\n",
    "    return x\n",
    "\n",
    "def bezier_x(p,t):\n",
    "    #given a t return corresponding x under bezier p\n",
    "    n = int(p.shape[1]/2)\n",
    "    b = bezier(n-1,t)\n",
    "    return torch.sum(b*p[:,:n],1)\n",
    "\n",
    "def bezier_t(p,x):\n",
    "    l = x.shape[0]\n",
    "    #given an x return correspoding t under bezier p\n",
    "    #interpolate\n",
    "    threshold =0.0001\n",
    "    max_loops=50\n",
    "    t_min=torch.zeros(l).to(device)\n",
    "    t_max=torch.ones(l).to(device)\n",
    "    while((max_loops>0) and (torch.sum(t_max-t_min)>=threshold)):\n",
    "        max_loops-=1\n",
    "        t_mid = (t_min+t_max)/2\n",
    "        x_mid = bezier_x(p,t_mid)\n",
    "        t_max = t_max*(x[:,0]>=x_mid)+t_mid*(x[:,0]<x_mid)\n",
    "        t_min = t_min*(x[:,0]<x_mid)+t_mid*(x[:,0]>=x_mid)\n",
    "    return t_mid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice_prob_index(a, axis=1):\n",
    "    r = np.expand_dims(np.random.rand(a.shape[1-axis]), axis=axis)\n",
    "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)\n",
    "\n",
    "def BDN_predict(x,percentile,denorm_fn):\n",
    "    pnt_np = np.ones((x.shape[0],1))*percentile\n",
    "    raw_result = bezier_inv_cdf(torch.from_numpy(pnt_np).type(torch.FloatTensor).to(device),torch.from_numpy(x).type(torch.FloatTensor).to(device))\n",
    "    result = denorm_fn(raw_result).to(device).numpy()\n",
    "    return result\n",
    "\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self,train_dates,eval_dates,df_in,cutoff_year):\n",
    "        self.all_valuation_dates = ['2019-12-31','2018-12-31','2017-12-31','2016-12-31','2015-12-31','2014-12-31','2013-12-31','2012-12-31','2011-12-31','2010-12-31','2009-12-31','2008-12-31','2007-12-31']\n",
    "        \n",
    "        #process the dataframe\n",
    "        df,symbols,marketcaps = self.__process_df__(train_dates,eval_dates,df_in,cutoff_year)\n",
    "        \n",
    "        #init with the df\n",
    "        self.model_X, self.model_Y, self.marketcaps_future,self.symbols = self.prepare_data(df,symbols,marketcaps)        \n",
    "                \n",
    "        #normalise the data\n",
    "        self.model_X,self.X_mean,self.X_min_max = self.normalise_X_data(self.model_X)\n",
    "        self.model_Y,self.Y_min,self.Y_min_max = self.normalise_Y_data(self.model_Y)\n",
    "        \n",
    "        #create a random sorted index    \n",
    "        self.rnd_idx = np.arange(self.symbols.shape[0])\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(self.rnd_idx)\n",
    "        \n",
    "        #shuffle the data\n",
    "        self.marketcaps_future = self.marketcaps_future[self.rnd_idx,:]\n",
    "        self.symbols = self.symbols[self.rnd_idx]\n",
    "        self.model_X = self.model_X[self.rnd_idx,:]\n",
    "        self.model_Y = self.model_Y[self.rnd_idx]        \n",
    "\n",
    "        \n",
    "    def __process_df__(self,train_dates,eval_dates,df,cutoff_year):\n",
    "        df_new = df.loc[df['Source_Year'] <cutoff_year].reset_index(drop=True)\n",
    "        #drop all entries that lack valuation data during the periods of interest\n",
    "        for dt in train_dates+eval_dates:\n",
    "            df_new = df_new.loc[df_new[dt] >0].reset_index(drop=True)\n",
    "        #store the eval date marketcaps seperatly\n",
    "        marketcaps = df_new[eval_dates]\n",
    "        #store the symbols seperatly\n",
    "        symbols = df_new['Symbol']\n",
    "        #remove all the columns we don't want to pass to the model for training\n",
    "        drop_valuation_dates = list(set(self.all_valuation_dates)-set(train_dates))\n",
    "        df_new = df_new.drop(columns=drop_valuation_dates+['Symbol'])\n",
    "        #create and return the dataset\n",
    "        return (df_new,symbols,marketcaps)\n",
    "    \n",
    "    def __get_norm_params__(self):\n",
    "        return ((self.X_mean,self.X_min_max),(self.Y_min,self.Y_min_max))\n",
    "\n",
    "    def normalise_X_data(self,model_data):\n",
    "        log_model_data = torch.log(torch.abs(model_data)+torch.ones(model_data.shape) )*torch.sign(model_data)\n",
    "        min_max_v = torch.max(torch.max(log_model_data,0)[0],0)[0] - torch.min(torch.min(log_model_data,0)[0],0)[0]\n",
    "        mean_v = torch.mean(torch.mean(log_model_data,0),0)\n",
    "        #ensure no divide by zero\n",
    "        min_max_v = min_max_v+ (min_max_v==0)\n",
    "        return ((log_model_data-mean_v)/min_max_v,mean_v,min_max_v)\n",
    "\n",
    "    def normalise_Y_data(self,model_data):\n",
    "        log_model_data = torch.log(model_data)\n",
    "        min_v =torch.min(torch.min(log_model_data,0)[0],0)[0]\n",
    "        min_max_v = torch.max(torch.max(log_model_data,0)[0],0)[0] - min_v\n",
    "        #ensure no divide by zero\n",
    "        min_max_v = min_max_v+ (min_max_v==0)\n",
    "        return ((log_model_data - min_v )/min_max_v,min_v,min_max_v)\n",
    "\n",
    "    def __denormalise__(self,Y_in):\n",
    "        return (torch.exp(Y_in*self.Y_min_max+self.Y_min))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.model_X[idx],self.model_Y[idx],self.marketcaps_future[idx],self.symbols[idx])\n",
    "\n",
    "    def prepare_data(self,df,symbols,marketcaps):\n",
    "        last_symbol=''\n",
    "        count=0\n",
    "        list_X =[]\n",
    "        list_Y =[]\n",
    "        list_marketcaps_future =[]\n",
    "        list_symbols = []\n",
    "        for idx in range(len(df)):\n",
    "            if(symbols[idx]!=last_symbol):\n",
    "                #process a new symbol\n",
    "                count=0\n",
    "                X_seq = []\n",
    "                Y = marketcaps.iloc[idx,0]\n",
    "                mcf = marketcaps.iloc[idx,1:].tolist()\n",
    "                sym = symbols[idx]\n",
    "            #process data\n",
    "            if (count>2):\n",
    "                #already have 3 entries\n",
    "                pass\n",
    "            else:\n",
    "                #process entry\n",
    "                X_seq.append(df.iloc[idx,:].to_list())\n",
    "            #increment the count        \n",
    "            count+=1\n",
    "            last_symbol = symbols[idx]\n",
    "            #store the sequence\n",
    "            if(count==3):\n",
    "                list_X.append(X_seq)\n",
    "                list_Y.append(Y)\n",
    "                list_marketcaps_future.append(mcf)\n",
    "                list_symbols.append(sym)\n",
    "        #convert to Log\n",
    "        return (torch.FloatTensor(list_X),torch.FloatTensor(list_Y),np.array(list_marketcaps_future),np.array(list_symbols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#load the data for the model\n",
    "############################\n",
    "\n",
    "df_data = pd.read_csv(model_data_path,sep='\\t')\n",
    "df_data.drop(columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "#one hot encodings\n",
    "onehot_cols=[]\n",
    "for col in df_data.columns:\n",
    "    if(df_data[col].dtype==np.object and col !=\"Symbol\"):\n",
    "        onehot_cols.append(col)\n",
    "df_data = pd.get_dummies(df_data,columns=onehot_cols,prefix=onehot_cols)\n",
    "\n",
    "#Build the validation dataset\n",
    "train_dates = []\n",
    "eval_dates = ['2009-12-31','2009-12-31','2010-12-31','2011-12-31','2012-12-31']\n",
    "cutoff_year = 2010\n",
    "dataset_validation = FinancialDataset(train_dates,eval_dates,df_data,cutoff_year)\n",
    "\n",
    "#Build the test dataset\n",
    "train_dates = []\n",
    "eval_dates = ['2016-12-31','2016-12-31','2017-12-31','2018-12-31','2019-12-31']\n",
    "cutoff_year = 2020\n",
    "dataset_test = FinancialDataset(train_dates,eval_dates,df_data,cutoff_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResults():\n",
    "    def __init__(self):\n",
    "        #a class to hold model results\n",
    "        self.df = None\n",
    "        self.Znames = []\n",
    "        self.Ynames = []\n",
    "        self.denorm_fn=None\n",
    "\n",
    "    def append(self,Y,Yhat,Z,Symbols,version,type_data,denorm_fn):\n",
    "        self.denorm_fn=denorm_fn\n",
    "        #create a dataframe and append it to the main dataframe \n",
    "        df_temp = pd.DataFrame(Y.cpu().numpy(),columns=['Y'])\n",
    "        Y_temp = Yhat.detach().cpu().numpy()\n",
    "        \n",
    "        self.Ynames = []\n",
    "        for i in range(Y_temp.shape[1]):\n",
    "            df_temp['Y_' + str(i)] = Y_temp[:,i]\n",
    "            self.Ynames.append('Y_' + str(i))\n",
    "\n",
    "        #create Z columns\n",
    "        Z_temp=Z.cpu().numpy()\n",
    "        \n",
    "        self.Znames = []\n",
    "        for i in range(Z_temp.shape[1]):\n",
    "            df_temp['Z_' + str(i)] = Z_temp[:,i]\n",
    "            self.Znames.append('Z_' + str(i))\n",
    "        \n",
    "        df_temp['Symbol'] = Symbols\n",
    "        df_temp['Version']=version\n",
    "        df_temp['Type']=type_data\n",
    "        \n",
    "        if (self.df is None):\n",
    "            self.df = df_temp\n",
    "        else:\n",
    "            self.df = self.df.append(df_temp)\n",
    "        return\n",
    "\n",
    "    def append_train(self,Y,Yhat,Z,Symbols,version,denorm_fn):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"train\",denorm_fn)\n",
    "        return\n",
    "\n",
    "    def append_val(self,Y,Yhat,Z,Symbols,version,denorm_fn):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"val\",denorm_fn)\n",
    "        return\n",
    "\n",
    "    def append_test(self,Y,Yhat,Z,Symbols,version,denorm_fn):\n",
    "        self.append(Y,Yhat,Z,Symbols,version,\"test\",denorm_fn)\n",
    "        return\n",
    "\n",
    "    def save_results(self,file_path):\n",
    "        self.df.to_csv(file_path,index=False)\n",
    "        \n",
    "    def get_all_predictions(self):\n",
    "        \n",
    "        #compute the average predictions for each symbol and return \n",
    "        df_avg = self.df.groupby(['Symbol']).mean().reset_index()\n",
    "        return (df_avg['Y'].to_numpy(),df_avg['Yhat'].to_numpy(),df_avg[self.Znames].to_numpy(),df_avg['Symbol'])\n",
    "    \n",
    "    def get_predictions_cap_band(self,cap_from,cap_to,percentile):\n",
    "        #compute the average predictions for each symbol and return \n",
    "        df_avg = self.df.copy()\n",
    "        #now compute the distibution values at the specified return period\n",
    "        #get the numpy distributions\n",
    "        #chunk the operation in max 1000 at a time\n",
    "        chunk_size=1000\n",
    "        df_avg['predictions']=0\n",
    "        for i in range(int(len(df_avg)/chunk_size)+1):\n",
    "            start_rng = i*chunk_size\n",
    "            end_rng = min(start_rng+chunk_size,len(df_avg))\n",
    "            \n",
    "            df_avg['predictions'].iloc[start_rng:end_rng]=BDN_predict((df_avg[self.Ynames].to_numpy())[start_rng:end_rng,:],percentile,self.denorm_fn).reshape(-1)\n",
    "        \n",
    "        df_avg['Y'] =self.denorm_fn(torch.from_numpy(df_avg['Y'].to_numpy()).type(torch.FloatTensor)).numpy()\n",
    "        #display(df_avg)        \n",
    "        df_avg = df_avg.groupby(['Symbol']).mean().reset_index()        \n",
    "        df_avg = df_avg.sort_values(by='Y',ascending=True).reset_index()\n",
    "        total_entries = len(df_avg)\n",
    "        idx_from = int(cap_from*total_entries)\n",
    "        idx_to = int(cap_to*total_entries)\n",
    "        df_avg = df_avg.iloc[idx_from:idx_to,:].reset_index()\n",
    "        return (df_avg['Y'].to_numpy(),df_avg['predictions'].to_numpy(),df_avg[self.Znames].to_numpy(),df_avg['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#### Use a neural network to fit the function\n",
    "#######################\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    " \n",
    "#######################\n",
    "#### Bezier Density Network\n",
    "#######################\n",
    "\n",
    "def Bezier_Output_Layer(x):\n",
    "    #we convert the final layer of the network into the Bezier outputs\n",
    "    with torch.no_grad():\n",
    "        #compute the number of control points\n",
    "        n=int((x.shape[1]+2)/2)\n",
    "        l = x.shape[0]\n",
    "    p_x = x[:,:(n-1)] \n",
    "    #constrain the x's\n",
    "    p_x =torch.exp(p_x)\n",
    "    p_x = torch.cumsum(p_x,1)/torch.sum(p_x,1).view(-1,1)\n",
    "    p_x0 = torch.zeros((l,1)).to(device)\n",
    "    p_z0 = torch.zeros((l,1)).to(device)\n",
    "    p_z = x[:,(n-1):(2*n-2)]    \n",
    "    #constrain the z's\n",
    "    p_z = torch.exp(p_z)\n",
    "    p_z = torch.cumsum(p_z,1)/torch.sum(p_z,1).view(-1,1)\n",
    "        \n",
    "    #apply formulas to allow more z flexability\n",
    "    p_x = torch.cat((p_x0,p_x),1)\n",
    "    p_z = torch.cat((p_z0,p_z),1)    \n",
    "    #we create an output of the form p_x p_z\n",
    "    p = torch.cat((p_x,p_z),1)\n",
    "    return p\n",
    "\n",
    "def Bezier_loss(x,y):\n",
    "    #we compute the MDN loss using a custom loss function\n",
    "    with torch.no_grad():\n",
    "        n=int(x.shape[1]/2)\n",
    "        batch_len=x.shape[0]\n",
    "    t=y.view(-1)\n",
    "    #compute the finitie difference of the cdf\n",
    "    prob = (-bezier_cdf(t-0.0001,x)+bezier_cdf(t+0.0001,x))/.0002\n",
    "    #take the log, with an additonal term to avoid log(0)\n",
    "    prob = torch.log((prob)*(prob>0)+(prob<=0))\n",
    "    loss=-torch.sum(prob  )/x.shape[0]\n",
    "    return loss\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 200\n",
    "        self.n_layers = 3\n",
    "\n",
    "        self.model = nn.ModuleDict({\n",
    "            'LSTM': nn.LSTM(213,self.hidden_dim,num_layers = self.n_layers,batch_first = True,dropout=0.5),\n",
    "            'Linear1': nn.Linear(200,50),\n",
    "            'ReLU': nn.ReLU(),\n",
    "            'Tanh': nn.Tanh(),\n",
    "            'Sigmoid': nn.Sigmoid(),\n",
    "            'Dropout': nn.Dropout(p=0.5),\n",
    "            'Linear2': nn.Linear(50,50),\n",
    "            'Linear3': nn.Linear(50,50),\n",
    "            'Linear4': nn.Linear(50,20*2-2)\n",
    "        })\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        #print(batch_size)\n",
    "        h0 = self.init_hidden(batch_size)\n",
    "        c0 = self.init_hidden(batch_size)\n",
    "        #print(hidden.shape)\n",
    "        output, (h_n,c_n) = self.model['LSTM'](x,(h0,c0))\n",
    "        x = output[:,-1,:]\n",
    "        x = self.model['Dropout'](x)\n",
    "        x = self.model['Linear1'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear2'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear3'](x)\n",
    "        x = self.model['Tanh'](x)\n",
    "        x = self.model['Linear4'](x)\n",
    "        x = Bezier_Output_Layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def run_model(train_dataset,val_dataset,net_results,run_version):\n",
    "    #train / val / test\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "    val_data_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=True)    \n",
    "    #run the model with this dataset\n",
    "    net = Net().to(device)\n",
    "    params = list(net.parameters())\n",
    "    criterion = Bezier_loss\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    #store the losses\n",
    "    loss_v=[]\n",
    "    loss_t=[]\n",
    "    \n",
    "    #moving average window\n",
    "    window=40\n",
    "    epoch=0\n",
    "    while ( (epoch < window) or ( np.mean( np.array(loss_v)[-window:] ) < np.mean( np.array(loss_v)[-(window+1):-1] ) )):\n",
    "    #for epoch in range(300):  # loop over the dataset multiple times (100 for test)\n",
    "        net.train()\n",
    "        t_loss=0\n",
    "        t_count=0\n",
    "        for batch_idx, (X, Y,_,_) in enumerate(train_data_loader):\n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X.to(device))\n",
    "            loss = criterion(outputs, Y.to(device))\n",
    "            t_loss+=loss.item()\n",
    "            t_count+=1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_t.append(t_loss/t_count) \n",
    "        #evaluate the loss on validation set    \n",
    "        v_loss=0\n",
    "        v_count=0\n",
    "        for batch_idx, (X_val, Y_val,_,_) in enumerate(val_data_loader):\n",
    "            v_loss+= criterion(net(X_val.to(device)), Y_val.to(device)).item()\n",
    "            v_count+=1\n",
    "        loss_v.append(v_loss/v_count)\n",
    "        #if epoch%100 ==0:    \n",
    "        print(f\"epoch = {epoch} train loss = {loss_t[-1]} validation loss = {loss_v[-1]}\")\n",
    "        epoch+=1\n",
    "        \n",
    "    print('Finished Training')\n",
    "    #put model into eval mode\n",
    "    net.eval()\n",
    "    plt.plot(loss_t, \"-b\", label=\"train\")\n",
    "    plt.plot(loss_v , \"-r\", label=\"validation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Train vs validation loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    #generate validation results\n",
    "    for batch_idx, (X_val, Y_val,Z,sym) in enumerate(val_data_loader):\n",
    "        net_results.append_val(\n",
    "            Y_val,\n",
    "            net(X_val.to(device)),\n",
    "            Z,\n",
    "            sym,\n",
    "            run_version,val_dataset.dataset.__denormalise__)\n",
    "\n",
    "    #generate train results\n",
    "    for batch_idx, (X_val, Y_val,Z,sym) in enumerate(train_data_loader):\n",
    "        net_results.append_train(\n",
    "            Y_val,\n",
    "            net(X_val.to(device)),\n",
    "            Z,\n",
    "            sym,\n",
    "            run_version,val_dataset.dataset.__denormalise__)\n",
    "    #now compute the results for the train and va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a random seed on the split \n",
    "torch.manual_seed(41)\n",
    "\n",
    "CV_Folds = 5\n",
    "CV_data =(None,)*CV_Folds\n",
    "split_sizes = [int(dataset_test.__len__()/CV_Folds)]*CV_Folds\n",
    "split_sizes[-1] = dataset_test.__len__() - (CV_Folds-1)*split_sizes[0]\n",
    "CV_data = torch.utils.data.random_split(dataset_test, split_sizes)\n",
    "CV_data = list(CV_data)\n",
    "net_results = ModelResults()\n",
    "\n",
    "#random runs for each fold\n",
    "Repeat = 1\n",
    "cnt =0\n",
    "for j in range(Repeat):\n",
    "    for i in range(CV_Folds):\n",
    "    \n",
    "        #take first element as validation and rest to train\n",
    "        print(f\"running fold {i+1} on repeat {j+1}\")\n",
    "        train_dataset = torch.utils.data.ConcatDataset(CV_data[1:])\n",
    "        val_dataset = CV_data[0]\n",
    "        print(f\"train dataset size ={train_dataset.__len__()} val dataset size = {val_dataset.__len__()}\")\n",
    "        run_model(train_dataset, val_dataset, net_results,cnt)\n",
    "        cnt+=1\n",
    "        #now change the order of the dataset\n",
    "        CV_data.append(CV_data.pop(0))\n",
    "\n",
    "#now switch to cpu\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create P&L charts\n",
    "def p_and_l_chart(return_dist,title):\n",
    "    Ys = np.sort(return_dist)\n",
    "    avg = np.mean(Ys)\n",
    "    mdn = np.median(Ys)\n",
    "    avg_log = np.log(1+avg*(avg>=0)) -np.log(1-avg*(avg<0))\n",
    "    Ys_pos = Ys*(Ys>=0)\n",
    "    Ys_neg = Ys*(Ys<0)\n",
    "    Ys_pos = np.log(1+Ys_pos)\n",
    "    Ys_neg = -np.log(1-Ys_neg)\n",
    "    labels = np.arange(Ys.shape[0])\n",
    "    \n",
    "    Y_t = np.array([-10,-1,0,1,10,100,1000,10000,100000,1000000])\n",
    "    Y_t_val = np.log(1+Y_t*(Y_t>=0)) -np.log(1-Y_t*(Y_t<0))\n",
    "    Y_t_labels = (Y_t*100).tolist()\n",
    "    Y_t_labels =[f\"{s:,.0f}%\" for s in Y_t_labels]\n",
    "    plt.yticks(Y_t_val, Y_t_labels)\n",
    "    plt.bar(labels,Ys_pos,width=1,color='green',label=\"Positive Return\")\n",
    "    plt.bar(labels,Ys_neg,width=1,color='red',label=\"Negative Return\")\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "    ax.plot([0., labels[-1]], [avg_log, avg_log], \"k--\",label=\"Mean Return = \" + f\"{avg*100:,.0f}%\")\n",
    "    ax.plot([Ys.shape[0]/2, Ys.shape[0]/2], [-np.log(2), np.log(2)], \"b--\",label=\"Median Return = \"+ f\"{mdn*100:,.0f}%\")\n",
    "    plt.ylabel('Compound Return % after 3 Years')\n",
    "    plt.xlabel('Companies - sorted from low to high return')\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "def training_v_actual_chart(Y,Y_hat,Ycheap,Y_hatcheap,title):\n",
    "    correl = np.corrcoef(Y,Y_hat)[1,0]\n",
    "    plt.scatter(Y, Y_hat, marker=\"o\",color='k',s=1.5,label=\"all companies\")\n",
    "    plt.scatter(Ycheap, Y_hatcheap, marker=\"o\",color='r',s=1.5,label=\"cheap companies\")\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"log(Actual Market Cap)\")\n",
    "    plt.ylabel(\"log(Predicted Market Cap)\")\n",
    "    plt.loglog()\n",
    "    axes = plt.gca()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def n_cheapest(Y,Yhat,n):\n",
    "    #return an array with n values = True for the largest differences\n",
    "    diff = Yhat/Y\n",
    "    #negate the sign to sortt descending\n",
    "    idxs = (-diff).argsort()\n",
    "    res = np.zeros(Y.shape).astype(bool)\n",
    "    \n",
    "    for i in range(n):\n",
    "        res[idxs[i]]=True#*(Yhat[idxs[i]]>Y[idxs[i]])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y, Y_hat, Z, Symbols = net_results.get_all_predictions()\n",
    "percentile =0.5\n",
    "bands = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "df_results = None\n",
    "for b in bands:\n",
    "    b_up=b+bands[1]\n",
    "    Y, Y_hat, Z, Symbols = net_results.get_predictions_cap_band(b,b+0.1,percentile)\n",
    "    y_ret=[]\n",
    "    y_cap=[]\n",
    "    for i in range(3):\n",
    "        y_ret.append((Z[:,i+1]/Z[:,i]-np.ones(Z.shape[0])))\n",
    "        y_cap.append(Z[:,i])\n",
    "\n",
    "    #create a dataframe with the results\n",
    "    df_temp = pd.DataFrame({'Results':f\"All {b:,.1f}-{b_up:,.1f}\", \n",
    "                               'Companies': Y.shape[0],\n",
    "                                'Yr1 Mean Return %': [f\"{100*np.mean(y_ret[0]):,.0f}%\"], \n",
    "                                'Yr2 Mean Return %': [f\"{100*np.mean(y_ret[1]):,.0f}%\"],                                        \n",
    "                                'Yr3 Mean Return %': [f\"{100*np.mean(y_ret[2]):,.0f}%\"], \n",
    "                              })\n",
    "    if df_results is None:\n",
    "        df_results = df_temp\n",
    "    else:\n",
    "        df_results = df_results.append(df_temp)\n",
    "    all_ret_dist = Z[:,-1]/Y - 1\n",
    "    p_and_l_chart(all_ret_dist,f\"3 Year Results - all companies - market caps {b:,.1f}-{b_up:,.1f}\")\n",
    "\n",
    "    #generate a return chart for different margin levels\n",
    "    cheapest = [40] #look at the 5% cheapest in each bracket of marketcap on test set and 25% on validation\n",
    "    for cheap in cheapest:\n",
    "        cheap_idx = n_cheapest(Y,Y_hat,cheap)\n",
    "        y_ret=[]\n",
    "        y_cap=[]\n",
    "        for i in range(3):\n",
    "            y_ret.append((Z[:,i+1]/Z[:,i]-np.ones(Z.shape[0]))[cheap_idx])\n",
    "            y_cap.append(Z[:,i][cheap_idx])\n",
    "        \n",
    "        #generate a scatter chart of the predictions vs actual market caps\n",
    "        training_v_actual_chart(Y,Y_hat,Y[cheap_idx],Y_hat[cheap_idx],f\"Actual vs Predicted Market Caps in {b:,.1f}-{b_up:,.1f}\")\n",
    "      \n",
    "        long_ret_dist = all_ret_dist[cheap_idx]\n",
    "        p_and_l_chart(long_ret_dist,f\"3 Year Results - cheapest {cheap:.0f} companies - market caps {b:,.1f}-{b_up:,.1f}\")\n",
    "        df_temp = pd.DataFrame({'Results':f\"Model {b:,.1f}-{b_up:,.1f} Cheapest = {cheap:.0f}\", \n",
    "                            'Companies': long_ret_dist.shape[0],\n",
    "                            'Yr1 Mean Return %': [f\"{100*np.mean(y_ret[0]):,.0f}%\"], \n",
    "                            'Yr2 Mean Return %': [f\"{100*np.mean(y_ret[1]):,.0f}%\"],                                        \n",
    "                            'Yr3 Mean Return %': [f\"{100*np.mean(y_ret[2]):,.0f}%\"], \n",
    "                           })\n",
    "        df_results = df_results.append(df_temp)\n",
    "net_results.save_results('test_results_bdn_model.csv')        \n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "p37workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
